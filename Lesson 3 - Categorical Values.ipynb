{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Variables\n",
    "## Introduction\n",
    "A **categorical variable** takes only a limited number of values.\n",
    "- Consider a survey that asks how often you eat breakfast and provides four options: \"Never\",\"Rarely\",\"Most days\", or \"Every day\". In this case, the data is categorical, because responses fall into a fixed set of categories.\n",
    "- If people responded to a survey about which brand of car they owned, the responses would fall into categories like \"Honda\",\"Toyota\", and \"Ford\". In this case, the data is also categorical.\n",
    "\n",
    "You will get an error if you try to plug these variables into most machine learning models in Python without preprocessing them first. In this tutorial, we'll compare three approaches that you can use to prepare your categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three Approaches\n",
    "### 1. Drop Categorical Variables\n",
    "The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information.\n",
    "### 2. Ordinal Encoding\n",
    "**Ordinal encoding** assigns each unique value to a different integer.\n",
    "This approach assumes an ordering of the categories: \"Never\" (0) < \"Rarely\" (1) < \"Most days\" (2) < \"Every day\" (3).\n",
    "\n",
    "This assumption makes sense in this example, because there is an indisputable ranking to the categories. Not all categorical variables have a clear ordering in the values, but we refer to those that do as **ordinal variables**. For tree-based models (like decision trees and random forests), you can expect ordinal encoding to work well with ordinal variables.\n",
    "\n",
    "### 3. One-Hot Encoding\n",
    "**One-hot encoding** creates new columns indicating the presence (or absence) of each possible value in the original data. To understand this, we'll work through an example.\n",
    "\n",
    "In the original dataset, \"Color\" is a categorical variable with three categories: \"Red\", \"Yellow\", and \"Green\". The corresponding one-hot encoding contains one column for each possible value, and one row for each row in the original dataset. Wherever the original value was \"Red\", we put a 1 in the \"Red\" column; if the original value was \"Yellow\", we put a 1 in the \"Yellow\" column, and so on.\n",
    "\n",
    "In contrast to ordinal encoding, one-hot encoding *does not* assume an ordering of the categories. Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data (e.g., \"Red\" is neither *more* nor *less* than \"Yellow\"). We refer to categorical variables without an intrinsic ranking as **nominal variables**.\n",
    "\n",
    "One-hot encoding generally does not perform well if the categorical variable taes on a large number of values (i.e., you generally won't use it for variables taking more than 15 different values).\n",
    "\n",
    "## Example\n",
    "As in the previous tutorial, we will work with the Melbourne Housing dataset.\n",
    "\n",
    "We won't focus on the data loading step. Instead, you can imagine you are at a point where you already have the training and validation data in `X_train`, `X_valid`, `y_train`, and `y_valid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate target from predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.Price\n",
    "X = data.drop(['Price'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide data into trianing and validation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns with missing values (simplest approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()]\n",
    "X_train_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_valid_full.drop(cols_with_missing, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Cardinality\" means the number of unique values in a column.\n",
    "\n",
    "Select categorical columns with relatively low cardinality (convenient but arbitrary),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == \"object\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select numerical columns,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep selected columns only,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cols = low_cardinality_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a peek at the training data with the `head()` method below."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
